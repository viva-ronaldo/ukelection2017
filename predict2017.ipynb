{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res17 = pd.read_csv('electdata_2017.txt',sep=';')\n",
    "res15 = pd.read_csv('electdata_2015.txt',sep=';')\n",
    "res10 = pd.read_csv('electdata_2010ug.txt',sep=';')\n",
    "res05 = pd.read_csv('electdata_2005nb.txt',sep=';')\n",
    "\n",
    "res17['year'] = 2017\n",
    "res15['year'] = 2015\n",
    "res10['year'] = 2010\n",
    "res05['year'] = 2005\n",
    "\n",
    "res17['winner'] = 'CON'\n",
    "res15['winner'] = 'CON'\n",
    "res10['winner'] = 'CON'\n",
    "res05['winner'] = 'CON'\n",
    "for party in ['LAB','LIB','NAT','MIN','OTH','Green','UKIP']:\n",
    "    res17.loc[(res17[party] >= res17.CON) & (res17[party] >= res17.LAB) & \\\n",
    "              (res17[party] >= res17.LIB) & (res17[party] >= res17.NAT) & \\\n",
    "              (res17[party] >= res17.Green) & (res17[party] >= res17.UKIP),'winner'] = party\n",
    "    res15.loc[(res15[party] >= res15.CON) & (res15[party] >= res15.LAB) & \\\n",
    "              (res15[party] >= res15.LIB) & (res15[party] >= res15.NAT) & \\\n",
    "              (res15[party] >= res15.Green) & (res15[party] >= res15.UKIP),'winner'] = party\n",
    "    res10.loc[(res10[party] >= res10.CON) & (res10[party] >= res10.LAB) & \\\n",
    "              (res10[party] >= res10.LIB) & (res10[party] >= res10.NAT) & \\\n",
    "              (res10[party] >= res10.Green) & (res10[party] >= res10.UKIP),'winner'] = party\n",
    "    if party not in ['UKIP','Green']:\n",
    "        res05.loc[(res05[party] >= res05.CON) & (res05[party] >= res05.LAB) & \\\n",
    "                  (res05[party] >= res05.LIB) & (res05[party] >= res05.NAT),'winner'] = party\n",
    "\n",
    "res17['Turnout_pc'] = (res17['CON']+res17['LAB']+res17['LIB']+res17['UKIP']+\n",
    "                       res17['NAT']+res17['Green']+res17['MIN']+res17['OTH']) / res17['Electorate']\n",
    "res15['Turnout_pc'] = (res15['CON']+res15['LAB']+res15['LIB']+res15['UKIP']+\n",
    "                       res15['NAT']+res15['Green']+res15['MIN']+res15['OTH']) / res15['Electorate']\n",
    "res10['Turnout_pc'] = (res10['CON']+res10['LAB']+res10['LIB']+res10['UKIP']+\n",
    "                       res10['NAT']+res10['Green']+res10['MIN']+res10['OTH']) / res10['Electorate']\n",
    "res05['Turnout_pc'] = (res05['CON']+res05['LAB']+res05['LIB']+\n",
    "                       res05['NAT']+res05['MIN']+res05['OTH']) / res05['Electorate']\n",
    "\n",
    "for party in ['CON','LAB','LIB','UKIP','Green','NAT','MIN','OTH']:\n",
    "    res17[party+'_pc'] = 100*res17[party] / (res17['Electorate']*res17['Turnout_pc'])\n",
    "    res15[party+'_pc'] = 100*res15[party]/ (res15['Electorate']*res15['Turnout_pc'])\n",
    "    res10[party+'_pc'] = 100*res10[party]/ (res10['Electorate']*res10['Turnout_pc'])\n",
    "    if party not in ['UKIP','Green']:\n",
    "        res05[party+'_pc'] = 100*res05[party] / (res05['Electorate']*res05['Turnout_pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res17 = pd.merge(res17, res15, on=['Name','Region','County Area'], how='inner', suffixes=('','_prev'))\n",
    "res15 = pd.merge(res15, res10, on=['Name','Region','County Area'], how='inner', suffixes=('','_prev'))\n",
    "res10 = pd.merge(res10, res05, on=['Name','Region','County Area'], how='inner', suffixes=('','_prev'))\n",
    "#Exclude NI regions because DUP are labelled as LIB which may be confusing\n",
    "res17 = res17[res17.Region != 3]\n",
    "res15 = res15[res15.Region != 3]\n",
    "res10 = res10[res10.Region != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = pd.concat([res15[list(res10.columns)],res10])\n",
    "test = res17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    632.000000\n",
      "mean      24.019887\n",
      "std       16.448468\n",
      "min        0.004782\n",
      "25%        9.590799\n",
      "50%       23.680349\n",
      "75%       34.851301\n",
      "max       77.140555\n",
      "Name: winner_majority_pc, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Calculate some more things\n",
    "def get_winner_pc_majority(row):\n",
    "    allpcs = row[['CON_pc','LAB_pc','LIB_pc','Green_pc','UKIP_pc','NAT_pc','MIN_pc','OTH_pc']].values\n",
    "    return row.winner_pc - np.sort(allpcs)[-2]\n",
    "\n",
    "training['winner_pc'] = training.apply(lambda row: row[row.winner+'_pc'],axis=1)\n",
    "test['winner_pc'] = test.apply(lambda row: row[row.winner+'_pc'],axis=1)\n",
    "training['winner_majority_pc'] = training.apply(get_winner_pc_majority, axis=1)\n",
    "test['winner_majority_pc'] = test.apply(get_winner_pc_majority, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 2017 accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "baseline_preds = test.winner_prev\n",
    "print 'Baseline 2017 accuracy: %.2f' % np.mean(baseline_preds == test.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier,Perceptron,LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   log reg: 0.854 (93% are prev winner)\n",
      "       sgd: 0.796 (87% are prev winner)\n",
      "      perc: 0.790 (85% are prev winner)\n",
      "       SVM: 0.443 (41% are prev winner)\n",
      "    linSVM: 0.801 (88% are prev winner)\n",
      "       KNN: 0.870 (94% are prev winner)\n",
      "     DTree: 0.839 (91% are prev winner)\n",
      "   RForest: 0.862 (95% are prev winner)\n",
      "    AdaB10: 0.829 (91% are prev winner)\n",
      "\n",
      "RF feature importances:\n",
      "    CON_pc_prev: 0.32\n",
      "    LAB_pc_prev: 0.21\n",
      "    NAT_pc_prev: 0.07\n",
      "    LIB_pc_prev: 0.09\n",
      "winner_prev_num: 0.21\n",
      "Turnout_pc_prev: 0.10\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(training['winner_prev'].append(test['winner_prev']))\n",
    "training['winner_prev_num'] = le.transform(training['winner_prev'])\n",
    "test['winner_prev_num'] = le.transform(test['winner_prev'])\n",
    "\n",
    "predictors = ['CON_pc_prev','LAB_pc_prev','NAT_pc_prev','LIB_pc_prev',\n",
    "                    'winner_prev_num','Turnout_pc_prev']\n",
    "X_train = training[predictors]\n",
    "y_train = training['winner']\n",
    "X_test = test[predictors]\n",
    "\n",
    "for method, clf in zip(['log reg','sgd','perc','SVM','linSVM','KNN','DTree','RForest','AdaB10'],\n",
    "               [LogisticRegression(), SGDClassifier(), Perceptron(), svm.SVC(), svm.LinearSVC(), \n",
    "                KNeighborsClassifier(n_neighbors=3), \n",
    "                tree.DecisionTreeClassifier(min_samples_split=10),\n",
    "                ensemble.RandomForestClassifier(n_estimators=100, min_samples_split=10),\n",
    "                ensemble.AdaBoostClassifier(n_estimators=10)]):\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    if method == 'RForest':\n",
    "        rfclf = clf\n",
    "        rfpreds1 = preds\n",
    "        rftrainpreds1 = clf.predict(X_train)\n",
    "    print '%10s: %.3f (%.0f%% are prev winner)' % (method,accuracy_score(preds, test.winner),\n",
    "                                                100*accuracy_score(preds, test.winner_prev))\n",
    "\n",
    "print '\\nRF feature importances:'\n",
    "for feature, importance in zip(X_train.columns,rfclf.feature_importances_):\n",
    "    print '%15s: %.2f' % (feature, importance)\n",
    "#There is no information beyond previous results, so best prediction is winner_prev\n",
    "#Best are Perceptron or Random Forest at 87-88% (baseline is 90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CON 34\n",
      "LAB -50\n",
      "LIB -10\n",
      "NAT 27\n",
      "UKIP 0\n",
      "Green -1\n",
      "\n",
      "2010, 2015 training total predicted seats (error):\n",
      "CON 646 (9)\n",
      "LAB 491 (5)\n",
      "LIB 51 (-12)\n",
      "NAT 68 (0)\n",
      "UKIP 0 (-1)\n",
      "Green 1 (-1)\n",
      "Accuracy = 0.951\n"
     ]
    }
   ],
   "source": [
    "#print rfpreds[rfpreds != res17.winner]\n",
    "for party in ['CON','LAB','LIB','NAT','UKIP','Green']:\n",
    "    print party, len([x for x in rfpreds1 if x == party]) - len([x for x in test.winner if x == party])\n",
    "#I overpredict CON and NAT and underpredict LAB\n",
    "#Polling will not help with this because the changes suggest more CON wins, and I have nothing on NAT\n",
    "\n",
    "print '\\n2010, 2015 training total predicted seats (error):'\n",
    "for party in ['CON','LAB','LIB','NAT','UKIP','Green']:\n",
    "    print '%s %i (%i)' % (party, len([x for x in rftrainpreds1 if x == party]), \n",
    "                          len([x for x in rftrainpreds1 if x == party]) - len([x for x in training.winner if x == party]))\n",
    "print 'Accuracy = %.3f' % accuracy_score(rftrainpreds1, training.winner)\n",
    "#In training I overestimate LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add opinion polling\n",
    "polls = pd.read_csv('finalpolls.csv')\n",
    "for party in ['CON','LAB','LIB']:\n",
    "    res17['pollChange'+party] = float(polls.loc[polls.year==2017,party]) - float(polls.loc[polls.year==2015,party])\n",
    "    res15['pollChange'+party] = float(polls.loc[polls.year==2015,party]) - float(polls.loc[polls.year==2010,party])\n",
    "    res10['pollChange'+party] = float(polls.loc[polls.year==2010,party]) - float(polls.loc[polls.year==2005,party])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   log reg: 0.532 (61% are prev winner)\n",
      "       sgd: 0.802 (87% are prev winner)\n",
      "      perc: 0.837 (91% are prev winner)\n",
      "       SVM: 0.415 (37% are prev winner)\n",
      "    linSVM: 0.491 (53% are prev winner)\n",
      "       KNN: 0.854 (94% are prev winner)\n",
      "     DTree: 0.835 (92% are prev winner)\n",
      "   RForest: 0.873 (95% are prev winner)\n",
      "    AdaB10: 0.829 (91% are prev winner)\n",
      "\n",
      "RF feature importances:\n",
      "    CON_pc_prev: 0.37\n",
      "    LAB_pc_prev: 0.16\n",
      "    NAT_pc_prev: 0.07\n",
      "    LIB_pc_prev: 0.08\n",
      "winner_prev_num: 0.19\n",
      "Turnout_pc_prev: 0.06\n",
      "  pollChangeCON: 0.03\n",
      "  pollChangeLAB: 0.03\n",
      "  pollChangeLIB: 0.02\n"
     ]
    }
   ],
   "source": [
    "training = pd.concat([res15[list(res10.columns)],res10])\n",
    "test = res17\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(training['winner_prev'].append(test['winner_prev']))\n",
    "training['winner_prev_num'] = le.transform(training['winner_prev'])\n",
    "test['winner_prev_num'] = le.transform(test['winner_prev'])\n",
    "\n",
    "predictors = ['CON_pc_prev','LAB_pc_prev','NAT_pc_prev','LIB_pc_prev',\n",
    "                    'winner_prev_num','Turnout_pc_prev',\n",
    "             'pollChangeCON','pollChangeLAB','pollChangeLIB']\n",
    "X_train = training[predictors]\n",
    "y_train = training['winner']\n",
    "X_test = test[predictors]\n",
    "\n",
    "for method, clf in zip(['log reg','sgd','perc','SVM','linSVM','KNN','DTree','RForest','AdaB10'],\n",
    "               [LogisticRegression(), SGDClassifier(), Perceptron(), svm.SVC(), svm.LinearSVC(), \n",
    "                KNeighborsClassifier(n_neighbors=3), \n",
    "                tree.DecisionTreeClassifier(min_samples_split=10),\n",
    "                ensemble.RandomForestClassifier(n_estimators=100, min_samples_split=10),\n",
    "                ensemble.AdaBoostClassifier(n_estimators=10)]):\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    if method == 'RForest':\n",
    "        rfclf = clf\n",
    "        rfpreds2 = preds\n",
    "        rftrainpreds2 = clf.predict(X_train)\n",
    "    print '%10s: %.3f (%.0f%% are prev winner)' % (method,accuracy_score(preds, test.winner),\n",
    "                                                100*accuracy_score(preds, test.winner_prev))\n",
    "\n",
    "print '\\nRF feature importances:'\n",
    "for feature, importance in zip(X_train.columns,rfclf.feature_importances_):\n",
    "    print '%15s: %.2f' % (feature, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CON 37\n",
      "LAB -46\n",
      "LIB -11\n",
      "NAT 21\n",
      "UKIP 0\n",
      "Green -1\n",
      "\n",
      "2010, 2015 training total predicted seats (error):\n",
      "CON 638 (1)\n",
      "LAB 485 (-1)\n",
      "LIB 59 (-4)\n",
      "NAT 74 (6)\n",
      "UKIP 0 (-1)\n",
      "Green 1 (-1)\n",
      "Accuracy = 0.959\n"
     ]
    }
   ],
   "source": [
    "#print rfpreds[rfpreds != res17.winner]\n",
    "for party in ['CON','LAB','LIB','NAT','UKIP','Green']:\n",
    "    print party, len([x for x in rfpreds2 if x == party]) - len([x for x in res17.winner if x == party])\n",
    "    \n",
    "print '\\n2010, 2015 training total predicted seats (error):'\n",
    "for party in ['CON','LAB','LIB','NAT','UKIP','Green']:\n",
    "    print '%s %i (%i)' % (party, len([x for x in rftrainpreds2 if x == party]), \n",
    "                          len([x for x in rftrainpreds2 if x == party]) - len([x for x in training.winner if x == party]))\n",
    "print 'Accuracy = %.3f' % accuracy_score(rftrainpreds2, training.winner)\n",
    "#Poll changes in 2010 and 2015 show LAB decreasing so removes positive bias of LAB predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes in 2017 preds after including polls:\n",
      "LAB CON\n",
      "LAB CON\n",
      "LAB CON\n",
      "LIB CON\n",
      "CON LIB\n",
      "NAT LAB\n",
      "NAT LAB\n",
      "NAT LAB\n",
      "NAT LAB\n",
      "NAT LAB\n",
      "NAT LAB\n",
      "NAT LAB\n",
      "LIB NAT\n"
     ]
    }
   ],
   "source": [
    "print 'Changes in 2017 preds after including polls:'\n",
    "for p1, p2 in zip(rfpreds1,rfpreds2):\n",
    "    if p1 != p2:\n",
    "        print p1,p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: different approach, use poll changes to model changes in vote percentages, then\n",
    "#  add these on to previous percentages to work out new winner. e.g. should probably\n",
    "#  predict Green win in BP.\n",
    "for party in ['CON','LAB','LIB']:\n",
    "    res17['voteChange'+party] = res17[party+'_pc'] - res17[party+'_pc_prev']\n",
    "    res15['voteChange'+party] = res15[party+'_pc'] - res15[party+'_pc_prev']\n",
    "    res10['voteChange'+party] = res10[party+'_pc'] - res10[party+'_pc_prev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CON % error baseline = -5.89, poll adjusted = 4.91\n",
      "Mean LAB % error baseline = -9.52, poll adjusted = -5.42\n",
      "Mean LIB % error baseline = 0.60, poll adjusted = -2.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "training = pd.concat([res15[list(res10.columns)],res10])\n",
    "test = res17\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# le.fit(training['winner_prev'].append(test['winner_prev']))\n",
    "# training['winner_prev_num'] = le.transform(training['winner_prev'])\n",
    "# test['winner_prev_num'] = le.transform(test['winner_prev'])\n",
    "\n",
    "predCON2017 = test['CON_pc_prev'] + test['pollChangeCON']\n",
    "predLAB2017 = test['LAB_pc_prev'] + test['pollChangeLAB']\n",
    "predLIB2017 = test['LIB_pc_prev'] + test['pollChangeLIB']\n",
    "\n",
    "print 'Mean CON %% error baseline = %.2f, poll adjusted = %.2f' % (np.mean(test['CON_pc_prev']-test['CON_pc']), np.mean(predCON2017-test['CON_pc']))\n",
    "print 'Mean LAB %% error baseline = %.2f, poll adjusted = %.2f' % (np.mean(test['LAB_pc_prev']-test['LAB_pc']), np.mean(predLAB2017-test['LAB_pc']))\n",
    "print 'Mean LIB %% error baseline = %.2f, poll adjusted = %.2f' % (np.mean(test['LIB_pc_prev']-test['LIB_pc']), np.mean(predLIB2017-test['LIB_pc']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline gets 568/650 seats right\n",
      "AddPoll gets 551/650 seats right\n"
     ]
    }
   ],
   "source": [
    "#Some functions for scoring\n",
    "def predPercentsAddPoll(row):\n",
    "    outrow = row\n",
    "    outrow['pred_CON_pc'] = min(max(outrow['CON_pc_prev'] + outrow['pollChangeCON'], 0), 100)\n",
    "    outrow['pred_LAB_pc'] = min(max(outrow['LAB_pc_prev'] + outrow['pollChangeLAB'], 0), 100)\n",
    "    outrow['pred_LIB_pc'] = min(max(outrow['LIB_pc_prev'] + outrow['pollChangeLIB'], 0), 100)\n",
    "    #big3scale = sum(outrow[['pred_CON_pc','pred_LAB_pc','pred_LIB_pc']].values) / sum(outrow[['CON_pc_prev','LAB_pc_prev','LIB_pc_prev']].values)\n",
    "    #print big3scale,sum(outrow[['pred_CON_pc','pred_LAB_pc','pred_LIB_pc']].values),sum(outrow[['CON_pc_prev','LAB_pc_prev','LIB_pc_prev']].values)\n",
    "    outrow['pred_Green_pc'] = outrow['Green_pc_prev'] \n",
    "    outrow['pred_UKIP_pc'] = outrow['UKIP_pc_prev'] \n",
    "    outrow['pred_NAT_pc'] = outrow['NAT_pc_prev'] \n",
    "    outrow['pred_MIN_pc'] = outrow['MIN_pc_prev'] \n",
    "    outrow['pred_OTH_pc'] = outrow['OTH_pc_prev'] \n",
    "    #print sum(outrow[['CON_pc','LAB_pc','LIB_pc','Green_pc','UKIP_pc','NAT_pc','MIN_pc','OTH_pc']].values)\n",
    "    divFact = sum(outrow[['pred_CON_pc','pred_LAB_pc','pred_LIB_pc','pred_Green_pc','pred_UKIP_pc','pred_NAT_pc','pred_MIN_pc','pred_OTH_pc']].values) / 100\n",
    "    for party in ['CON','LAB','LIB','Green','UKIP','NAT','MIN','OTH']:\n",
    "        outrow['pred_'+party+'_pc'] /= divFact\n",
    "    #print sum(outrow[['pred_CON_pc','pred_LAB_pc','pred_LIB_pc','pred_Green_pc','pred_UKIP_pc','pred_NAT_pc','pred_MIN_pc','pred_OTH_pc']].values)\n",
    "    return outrow\n",
    "\n",
    "def getWinner(row, getPercentsMethod):\n",
    "    outDF = getPercentsMethod(row)\n",
    "    winIndex = np.argmax(outDF[['pred_CON_pc','pred_LAB_pc','pred_LIB_pc','pred_Green_pc','pred_UKIP_pc','pred_NAT_pc','pred_MIN_pc','pred_OTH_pc']].values)\n",
    "    return ['CON','LAB','LIB','Green','UKIP','NAT','MIN','OTH'][winIndex]\n",
    "\n",
    "def getWinnerBaseline(test):\n",
    "    winIndex = np.argmax(test[['CON_pc_prev','LAB_pc_prev','LIB_pc_prev','Green_pc_prev','NAT_pc_prev','MIN_pc_prev','OTH_pc_prev']].values)\n",
    "    return ['CON','LAB','LIB','Green','NAT','MIN','OTH'][winIndex]\n",
    "\n",
    "#\n",
    "def predictSeats(test):\n",
    "    winners = test.apply(getWinner,axis=1,args=[predPercentsAddPoll])\n",
    "    for party in ['CON','LAB','LIB','Green','NAT','MIN','OTH']:\n",
    "        nSeats = len([p for p in winners if p == party])\n",
    "        if nSeats > 0:\n",
    "            print party,nSeats\n",
    "\n",
    "def predictAndEvaluate(test, method):\n",
    "    if method == 'baseline':\n",
    "        predWinners = test.apply(getWinnerBaseline,axis=1)\n",
    "    else:\n",
    "        predWinners = test.apply(getWinner,axis=1,args=[method])\n",
    "    trueWinners = test.winner\n",
    "    nRight = 0\n",
    "    for i in range(len(trueWinners)):\n",
    "        if predWinners[i] == trueWinners[i]:\n",
    "            nRight += 1\n",
    "    return nRight\n",
    "\n",
    "print 'Baseline gets %i/650 seats right' % predictAndEvaluate(test,'baseline')\n",
    "print 'AddPoll gets %i/650 seats right' % predictAndEvaluate(test,predPercentsAddPoll)\n",
    "#TODO: breakdown accuracy by party, winner_majority_pc_prev?\n",
    "\n",
    "#TODO could use a model to better apply national poll changes to individual constituencies, e.g. grouped by winner_pc_majority or party vote pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Lab % MAE:  9.67627063736\n",
      "AddPoll Lab % MAE:  9.59070117354\n"
     ]
    }
   ],
   "source": [
    "#print test.head(12).apply(getWinner,axis=1,args=[predPercentsAddPoll])\n",
    "print 'Baseline Lab % MAE: ',metrics.mean_absolute_error(test.LAB_pc_prev, test.LAB_pc)\n",
    "print 'AddPoll Lab % MAE: ',metrics.mean_absolute_error(test.apply(predPercentsAddPoll,axis=1).pred_LAB_pc, test.LAB_pc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
